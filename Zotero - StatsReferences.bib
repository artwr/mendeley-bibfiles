Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Casella1992,
abstract = {Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.},
author = {Casella, George and George, Edward I.},
doi = {10.2307/2685208},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/S2ZTUQIA/Casella and George - 1992 - Explaining the Gibbs Sampler.pdf:pdf},
issn = {00031305},
journal = {The American Statistician},
month = aug,
number = {3},
pages = {167},
title = {{Explaining the Gibbs Sampler}},
url = {http://www.jstor.org/stable/2685208 http://www.jstor.org/stable/pdfplus/2685208.pdf?acceptTC=true http://www.jstor.org/stable/2685208?origin=crossref},
volume = {46},
year = {1992}
}
@article{Arlot2010d,
abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Model selection,cross-validation,leave-one-out},
language = {EN},
mendeley-tags = {Model selection,cross-validation,leave-one-out},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Goodman1960,
abstract = {Abstract A simple exact formula for the variance of the product of two random variables, say, x and y, is given as a function of the means and central product-moments of x and y. The usual approximate variance formula for xy is compared with this exact formula; e.g., we note, in the special case where x and y are independent, that the ?variance? computed by the approximate formula is less than the exact variance, and that the accuracy of the approximation depends on the sum of the reciprocals of the squared coefficients of variation of x and y. The case where x and y need not be independent is also studied, and exact variance formulas are presented for several different ?product estimates.? (The usefulness of exact formulas becomes apparent when the variances of these estimates are compared.) When x and y are independent, simple unbiased estimates of these exact variances are suggested; in the more general case, consistent estimates are presented.
Abstract A simple exact formula for the variance of the product of two random variables, say, x and y, is given as a function of the means and central product-moments of x and y. The usual approximate variance formula for xy is compared with this exact formula; e.g., we note, in the special case where x and y are independent, that the ?variance? computed by the approximate formula is less than the exact variance, and that the accuracy of the approximation depends on the sum of the reciprocals of the squared coefficients of variation of x and y. The case where x and y need not be independent is also studied, and exact variance formulas are presented for several different ?product estimates.? (The usefulness of exact formulas becomes apparent when the variances of these estimates are compared.) When x and y are independent, simple unbiased estimates of these exact variances are suggested; in the more general case, consistent estimates are presented.},
author = {Goodman, Leo A.},
doi = {10.1080/01621459.1960.10483369},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = dec,
number = {292},
pages = {708--713},
publisher = {Taylor \& Francis},
title = {{On the Exact Variance of Products}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1960.10483369},
volume = {55},
year = {1960}
}
@article{Clogg1995,
abstract = {Statistical methods are developed for comparing regression coefficients between models in the setting where one of the models is nested in the other. Comparisons of this kind are of interest whenever two explanations of a given phenomenon are specified as linear models. In this case, researchers should ask whether the coefficients associated with a given set of predictors change in a significant way when other predictors or covariates are added as controls. Simple calculations based on quantities provided by routines for regression analysis can be used to obtain the standard errors and other statistics that are required. Results are also given for the class of generalized linear models (e.g., logistic regression, log-linear models, etc.). We recommend fundamental change in strategies for model comparison in social research as well as modifications in the presentation of results from regression or regression-type models.},
author = {Clogg, Clifford C. and Petkova, Eva and Haritou, Adamantios},
doi = {10.2307/2782277},
issn = {0002-9602},
journal = {American Journal of Sociology},
month = mar,
number = {5},
pages = {1261--1293},
title = {{Statistical Methods for Comparing Regression Coefficients Between Models}},
url = {http://www.jstor.org/stable/2782277 http://www.jstor.org/stable/pdfplus/2782277.pdf?acceptTC=true},
volume = {100},
year = {1995}
}
@article{Li2013,
abstract = {Numerical models for variable-density flow and solute transport (VDFST) are widely used to simulate seawater intrusion and related problems. The mathematical model for VDFST is a coupled nonlinear dynamical system, so the numerical discretizations in time and space are usually required to be as fine as possible. As a result, fine-scale transient models require large computational time, which is a disadvantage for state estimation, forward prediction or model inversion. The purpose of this research is to develop mathematical and numerical methods to simulate VDFST via a model order reduction technique called Proper Orthogonal Decomposition (POD) designed for nonlinear dynamical systems. POD was applied to extract leading “model features” (basis functions) through singular value decomposition (SVD) from observational data or simulations (snapshots) of high-dimensional systems. These basis functions were then used in the Galerkin projection procedure that yielded low-dimensional (reduced-order) models. The original full numerical models were also discretized by the Galerkin Finite-Element Method (GFEM). The implementation of the POD reduced-order method was straightforward when applied to the full order model to the complex model. The developed GFEM-POD model was applied to solve two classic VDFST cases, the Henry problem and the Elder problem, in order to investigate the accuracy and efficiency of the POD model reduction method. Once the snapshots from full model results are obtained, the reduced-order model can reproduce the full model results with acceptable accuracy but with less computational cost in comparison with the full model, which is useful for model calibration and data assimilation problems. We found that the accuracy and efficiency of the POD reduced-order model is mainly determined by the optimal selection of snapshots and POD bases. Validation and verification experiments confirmed our POD model reduction procedure.},
author = {Li, Xinya and Chen, Xiao and Hu, Bill X. and Navon, I. Michael},
doi = {10.1016/j.jhydrol.2013.09.011},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/RMNCFWSJ/Li et al. - 2013 - Model reduction of a coupled numerical model using.pdf:pdf},
issn = {0022-1694},
journal = {Journal of Hydrology},
keywords = {Galerkin projection,Model reduction,Proper orthogonal decomposition,Single value decomposition,Variable density flow},
mendeley-tags = {Galerkin projection,Model reduction,Proper orthogonal decomposition,Single value decomposition,Variable density flow},
month = dec,
pages = {227--240},
title = {{Model reduction of a coupled numerical model using proper orthogonal decomposition}},
url = {http://www.sciencedirect.com/science/article/pii/S0022169413006562 http://www.sciencedirect.com/science/article/pii/S0022169413006562/pdfft?md5=628713cff94d3a13e1d75748e7ee02b8\&pid=1-s2.0-S0022169413006562-main.pdf},
volume = {507},
year = {2013}
}
@article{Tonkin2009,
abstract = {We describe a subspace Monte Carlo (SSMC) technique that reduces the burden of calibration-constrained Monte Carlo when undertaken with highly parameterized models. When Monte Carlo methods are used to evaluate the uncertainty in model outputs, ensuring that parameter realizations reproduce the calibration data requires many model runs to condition each realization. In the new SSMC approach, the model is first calibrated using a subspace regularization method, ideally the hybrid Tikhonov-TSVD “superparameter” approach described by Tonkin and Doherty (2005). Sensitivities calculated with the calibrated model are used to define the calibration null-space, which is spanned by parameter combinations that have no effect on simulated equivalents to available observations. Next, a stochastic parameter generator is used to produce parameter realizations, and for each a difference is formed between the stochastic parameters and the calibrated parameters. This difference is projected onto the calibration null-space and added to the calibrated parameters. If the model is no longer calibrated, parameter combinations that span the calibration solution space are reestimated while retaining the null-space projected parameter differences as additive values. The recalibration can often be undertaken using existing sensitivities, so that conditioning requires only a small number of model runs. Using synthetic and real-world model applications we demonstrate that the SSMC approach is general (it is not limited to any particular model or any particular parameterization scheme) and that it can rapidly produce a large number of conditioned parameter sets.},
author = {Tonkin, Matthew and Doherty, John},
doi = {10.1029/2007WR006678},
issn = {1944-7973},
journal = {Water Resources Research},
keywords = {Monte Carlo,calibration,parameter estimation,uncertainty analysis},
language = {en},
mendeley-tags = {Monte Carlo,calibration,parameter estimation,uncertainty analysis},
number = {12},
pages = {n/a--n/a},
title = {{Calibration-constrained Monte Carlo analysis of highly parameterized models using subspace techniques}},
url = {http://onlinelibrary.wiley.com/doi/10.1029/2007WR006678/abstract http://onlinelibrary.wiley.com/store/10.1029/2007WR006678/asset/wrcr11640.pdf?v=1\&t=hfgcwafb\&s=339c119289516dd9c0966a3e326a76f221af6ae5},
volume = {45},
year = {2009}
}
@article{Yen1956a,
abstract = {The purpose of this investigation is to examine four special nonuniform sampling processes in detail, and to deduce some interesting properties of bandwidth-limited signals. The main results are contained in four generalized sampling theorems. These theorems not only contain the nature of determination (unique-specification, over-specification, and underspecification) of signals but also include explicit reconstruction formulas. From the reconstruction formulas, the complexity and accuracy of the nonuniform sampling processes discussed can be estimated. In addition, these theorems lead to observations regarding the allowable shapes, the "prediction," and the "energy" of bandwidth-limited signals in general. A "minimum-energy" signal is introduced which has certain advantages as compared to the ordinary "time-limited" signals when a finite number of sample values are given. Finally, a statement due to Cauchy on the sampling of bandwidth-limited signals is generalized to include a wider class of nonuniform sample point distributions and modified to give more exact information regarding the nature of determination of signals.},
author = {Yen, J.},
doi = {10.1109/TCT.1956.1086325},
issn = {0096-2007},
journal = {IRE Transactions on Circuit Theory},
keywords = {Bridges,Frequency conversion,Mathematics,Nonuniform sampling,Physics,Sampling methods,Signal sampling,Telegraphy,interpolation,signal processing},
mendeley-tags = {Bridges,Frequency conversion,Mathematics,Nonuniform sampling,Physics,Sampling methods,Signal sampling,Telegraphy,interpolation,signal processing},
number = {4},
pages = {251--257},
title = {{On Nonuniform Sampling of Bandwidth-Limited Signals}},
url = {http://ieeexplore.ieee.org/ielx5/8148/23600/01086325.pdf?tp=\&arnumber=1086325\&isnumber=23600 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1086325\&sortType=asc\_p\_Sequence\&filter=AND(p\_IS\_Number:23600)},
volume = {3},
year = {1956}
}
@book{Pearson2011,
abstract = {The recent dramatic rise in the number of public datasets available free from the Internet, coupled with the evolution of the Open Source software movement, which makes powerful analysis packages like R freely available, have greatly increased both the range of opportunities for exploratory data analysis and the variety of tools that support this type of analysis. This book will provide a thorough introduction to a useful subset of these analysis tools, illustrating what they are, what they do, and when and how they fail. Specific topics covered include descriptive characterizations like summary statistics (mean, median, standard deviation, MAD scale estimate), graphical techniques like boxplots and nonparametric density estimates, various forms of regression modeling (standard linear regression models, logistic regression, and highly robust techniques like least trimmed squares), and the recognition and treatment of important data anomalies like outliers and missing data. The unique combination of topics presented in this book separate it from any other book of its kind. Intended for use as an introductory textbook for an exploratory data analysis course or as self-study companion for professionals and graduate students, this book assumes familiarity with calculus and linear algebra, though no previous exposure to probability or statistics is required. Both simulation-based and real data examples are included, as are end-of-chapter exercises and both R code and datasets.},
author = {Pearson, Ronald K.},
isbn = {9780195089653},
keywords = {Business \& Economics / Statistics,Engineering,Mathematical statistics,Mathematical statistics/ Textbooks,Mathematics / Probability \& Statistics / General,Medical sciences,Science,Technology \& Engineering / Engineering (General)},
language = {en},
mendeley-tags = {Business \& Economics / Statistics,Engineering,Mathematical statistics,Mathematical statistics/ Textbooks,Mathematics / Probability \& Statistics / General,Medical sciences,Science,Technology \& Engineering / Engineering (General)},
month = jan,
pages = {798},
publisher = {Oxford University Press},
title = {{Exploring data in engineering, the sciences, and medicine}},
url = {http://books.google.com/books?id=nH4pAQAAMAAJ},
year = {2011}
}
@article{Shao1993,
abstract = {We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular leave-one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaike information criterion (AIC), the Cp, and the bootstrap, is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive ability does not converge to 1 as the total number of observations n → ∞. We show that the inconsistency of the leave-one-out cross-validation can be rectified by using a leave-n$\nu$-out cross-validation with n$\nu$, the number of observations reserved for validation, satisfying n$\nu$/n → 1 as n → ∞. This is a somewhat shocking discovery, because n$\nu$/n → 1 is totally opposite to the popular leave-one-out recipe in cross-validation. Motivations, justifications, and discussions of some practical aspects of the use of the leave-n$\nu$-out cross-validation method are provided, and results from a simulation study are presented.},
author = {Shao, Jun},
doi = {10.2307/2290328},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/K86T86ZC/Shao - 1993 - Linear Model Selection by Cross-Validation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = jun,
number = {422},
pages = {486--494},
title = {{Linear Model Selection by Cross-Validation}},
url = {http://www.jstor.org/stable/2290328 http://www.jstor.org/stable/pdfplus/2290328.pdf?acceptTC=true},
volume = {88},
year = {1993}
}
@article{MacKinnon1985,
abstract = {We examine several modified versions of the heteroskedasticity-consistent covariance matrix estimator of Hinkley (1977) and White (1980). On the basis of sampling experiments which compare the performance of quasi t-statistics, we find that one estimator, based on the jackknife, performs better in small samples than the rest. We also examine the finite-sample properties of using modified critical values based on Edgeworth approximations, as proposed by Rothenberg (1984). In addition, we compare the power of several tests for heteroskedasticity, and find that it may be wise to employ the jackknife heteroskedasticity-consistent covariance matrix even in the absence of detected heteroskedasticity.},
author = {MacKinnon, James G and White, Halbert},
doi = {10.1016/0304-4076(85)90158-7},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/ETPNSX4W/MacKinnon and White - 1985 - Some heteroskedasticity-consistent covariance matr.html:html;:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/GFF9R9UF/MacKinnon and White - 1985 - Some heteroskedasticity-consistent covariance matr.html:html;:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/HS3DJBN7/MacKinnon and White - 1985 - Some heteroskedasticity-consistent covariance matr.pdf:pdf;:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/KRRBBD4V/MacKinnon and White - 1985 - Some heteroskedasticity-consistent covariance matr.pdf:pdf;:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/RQ8KWU9A/MacKinnon and White - 1985 - Some heteroskedasticity-consistent covariance matr.html:html;:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/XKRB8Z2P/MacKinnon and White - 1985 - Some heteroskedasticity-consistent covariance matr.pdf:pdf},
issn = {0304-4076},
journal = {Journal of Econometrics},
month = sep,
number = {3},
pages = {305--325},
title = {{Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties}},
url = {http://www.sciencedirect.com/science/article/pii/0304407685901587 http://www.sciencedirect.com/science/article/pii/0304407685901587/pdf?md5=3699e644616ccc750b6906e6d52bb3c6\&pid=1-s2.0-0304407685901587-main.pdf},
volume = {29},
year = {1985}
}
@article{Goldberger1962,
abstract = {When interdependence of disturbances is present in a regression model, the pattern of sample residuals contains information which is useful in prediction of post-sample drawings. This information, which is often overlooked, is exploited in the best linear unbiased predictor derived here. The gain in efficiency associated with using this predictor instead of the usual expected value estimator may be substantial.},
author = {Goldberger, Arthur S.},
doi = {10.2307/2281645},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/QV3XPZ84/Goldberger - 1962 - Best Linear Unbiased Prediction in the Generalized.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = jun,
number = {298},
pages = {369--375},
title = {{Best Linear Unbiased Prediction in the Generalized Linear Regression Model}},
url = {http://www.jstor.org/stable/2281645 http://www.jstor.org/stable/pdfplus/2281645.pdf?acceptTC=true},
volume = {57},
year = {1962}
}
@article{Belcher1994,
abstract = {An increasingly valuable tool for modelling irregularly sampled time series data is the continuous time autoregressive model. The natural parameters in this model are the coefficients of the linear stochastic differential equation describing the process which gives rise to the data. A transformation of these parameters is introduced, based on the Cayley-Hamilton transformation. The new parameter space is identical with that of discrete time autoregressive models. The model is also modified by the introduction of prescribed moving average terms. The resulting modelling improvements include rapid and reliable convergence of parameter estimates and the ability to select the model order by testing whether the highest order coefficient is 0. A geophysical and a medical application illustrate the detection of periodicities in data by using the spectrum of the fitted model.},
author = {Belcher, J. and Hampton, J. S. and Wilson, G. Tunnicliffe},
issn = {00359246},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
language = {English},
number = {1},
pages = {pp. 141--155},
title = {{Parameterization of Continuous Time Autoregressive Models for Irregularly Sampled Time Series Data}},
url = {http://www.jstor.org/stable/2346034},
volume = {56},
year = {1994}
}
@article{Beauchamp1973,
abstract = {Experience with biological data, such as dimensions of organisms, often confirms that logarithmic transformations should precede the testing of hypotheses about regression relations. However, estimates also may be needed in terms of untransformed variables. Just taking antilogarithms of values from a log-log regression line or function leads to biased estimates. This note compares corrections for this bias, and includes an example relating mass of tree parts (bole, branches, and leaves) to tree diameter of tulip poplar (Liriodendron tulipifera L.) in Oak Ridge, Tennessee, forests. An Appendix summarizes derivation of exact and approximate unbiased estimators of expected values from log-antilog regression, and of variance around the unbiased regression line.},
author = {Beauchamp, John J. and Olson, Jerry S.},
doi = {10.2307/1934208},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/CBK7RZ6R/Beauchamp and Olson - 1973 - Corrections for Bias in Regression Estimates After.pdf:pdf;:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/33AR8QBX/1934208.html:html},
issn = {0012-9658},
journal = {Ecology},
month = nov,
number = {6},
pages = {1403--1407},
title = {{Corrections for Bias in Regression Estimates After Logarithmic Transformation}},
url = {http://www.jstor.org/stable/1934208 http://www.jstor.org/discover/10.2307/1934208?uid=3739560\&uid=2\&uid=4\&uid=3739256\&sid=21103119743113 http://www.jstor.org/stable/pdfplus/1934208.pdf?acceptTC=true},
volume = {54},
year = {1973}
}
@article{Arlot2010c,
abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Model selection,cross-validation,leave-one-out},
language = {EN},
mendeley-tags = {Model selection,cross-validation,leave-one-out},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Efron1997,
abstract = {A training set of data has been used to construct a rule for predicting future responses. What is the error rate of this rule? This is an important question both for comparing models and for assessing a final selected model. The traditional answer to this question is given by cross-validation. The cross-validation estimate of prediction error is nearly unbiased but can be highly variable. Here we discuss bootstrap estimates of prediction error, which can be thought of as smoothed versions of cross-validation. We show that a particular bootstrap method, the .632+ rule, substantially outperforms cross-validation in a catalog of 24 simulation experiments. Besides providing point estimates, we also consider estimating the variability of an error rate estimate. All of the results here are nonparametric and apply to any possible prediction rule; however, we study only classification problems with 0-1 loss in detail. Our simulations include "smooth" prediction rules like Fisher's linear discriminant function and unsmooth ones like nearest neighbors.},
author = {Efron, Bradley and Tibshirani, Robert},
doi = {10.2307/2965703},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/ZCM7X43R/Efron and Tibshirani - 1997 - Improvements on Cross-Validation The .632+ Bootst.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = jun,
number = {438},
pages = {548--560},
shorttitle = {Improvements on Cross-Validation},
title = {{Improvements on Cross-Validation: The .632+ Bootstrap Method}},
url = {http://www.jstor.org/stable/2965703 http://www.jstor.org/stable/pdfplus/2965703.pdf?acceptTC=true},
volume = {92},
year = {1997}
}
@article{Hurvich1998,
abstract = {Many different methods have been proposed to construct nonparametric estimates of a smooth regression function, including local polynomial, (convolution) kernel and smoothing spline estimators. Each of these estimators uses a smoothing parameter to control the amount of smoothing performed on a given data set. In this paper an improved version of a criterion based on the Akaike information criterion (AIC), termed AICC, is derived and examined as a way to choose the smoothing parameter. Unlike plug-in methods, AICC can be used to choose smoothing parameters for any linear smoother, including local quadratic and smoothing spline estimators. The use of AICC avoids the large variability and tendency to undersmooth (compared with the actual minimizer of average squared error) seen when other ‘classical’ approaches (such as generalized cross-validation (GCV) or the AIC) are used to choose the smoothing parameter. Monte Carlo simulations demonstrate that the AICC-based smoothing parameter is competitive with a plug-in method (assuming that one exists) when the plug-in method works well but also performs well when the plug-in approach fails or is unavailable.},
author = {Hurvich, Clifford M. and Simonoff, Jeffrey S. and Tsai, Chih-Ling},
doi = {10.1111/1467-9868.00125},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/AEK3EZMB/Hurvich et al. - 1998 - Smoothing parameter selection in nonparametric reg.pdf:pdf},
issn = {1467-9868},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Convolution kernel regression estimator,Local polynomial regression estimator,Plug-in method,Smoothing spline regression estimator},
language = {en},
mendeley-tags = {Convolution kernel regression estimator,Local polynomial regression estimator,Plug-in method,Smoothing spline regression estimator},
number = {2},
pages = {271--293},
title = {{Smoothing parameter selection in nonparametric regression using an improved Akaike information criterion}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00125/abstract http://onlinelibrary.wiley.com/store/10.1111/1467-9868.00125/asset/1467-9868.00125.pdf?v=1\&t=hru19739\&s=af7072a8433c950538b37cfa64e36546d256af79},
volume = {60},
year = {1998}
}
@article{Molinaro2005,
abstract = {Motivation: In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the ‘true’ prediction error of a prediction model in the presence of feature selection.
Results: For small studies where features are selected from thousands of candidates, the resubstitution and simple split-sample estimates are seriously biased. In these small samples, leave-one-out cross-validation (LOOCV), 10-fold cross-validation (CV) and the .632+ bootstrap have the smallest bias for diagonal discriminant analysis, nearest neighbor and classification trees. LOOCV and 10-fold CV have the smallest bias for linear discriminant analysis. Additionally, LOOCV, 5- and 10-fold CV, and the .632+ bootstrap have the lowest mean square error. The .632+ bootstrap is quite biased in small sample sizes with strong signal-to-noise ratios. Differences in performance among resampling methods are reduced as the number of specimens available increase.
Contact: annette.molinaro@yale.edu
Supplementary Information: A complete compilation of results and R code for simulations and analyses are available in Molinaro et al. (2005) (http://linus.nci.nih.gov/brb/TechReport.htm).},
author = {Molinaro, Annette M. and Simon, Richard and Pfeiffer, Ruth M.},
doi = {10.1093/bioinformatics/bti499},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/DGVV4GUS/Molinaro et al. - 2005 - Prediction error estimation a comparison of resam.pdf:pdf},
issn = {1367-4803, 1460-2059},
journal = {Bioinformatics},
language = {en},
month = aug,
number = {15},
pages = {3301--3307},
shorttitle = {Prediction error estimation},
title = {{Prediction error estimation: a comparison of resampling methods}},
url = {http://bioinformatics.oxfordjournals.org/content/21/15/3301 http://bioinformatics.oxfordjournals.org/content/21/15/3301.full.pdf http://www.ncbi.nlm.nih.gov/pubmed/15905277},
volume = {21},
year = {2005}
}
@article{Anscombe1973,
author = {Anscombe, F. J.},
doi = {10.2307/2682899},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/NQFSE474/Anscombe - 1973 - Graphs in Statistical Analysis.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {graphing,graphs,statistical analysis},
mendeley-tags = {graphing,graphs,statistical analysis},
month = feb,
number = {1},
pages = {17--21},
title = {{Graphs in Statistical Analysis}},
url = {http://www.jstor.org/stable/2682899 http://www.jstor.org/stable/pdfplus/2682899.pdf?acceptTC=true},
volume = {27},
year = {1973}
}
@article{Kim2009a,
abstract = {We consider the accuracy estimation of a classifier constructed on a given training sample. The naive resubstitution estimate is known to have a downward bias problem. The traditional approach to tackling this bias problem is cross-validation. The bootstrap is another way to bring down the high variability of cross-validation. But a direct comparison of the two estimators, cross-validation and bootstrap, is not fair because the latter estimator requires much heavier computation. We performed an empirical study to compare the .632+ bootstrap estimator with the repeated 10-fold cross-validation and the repeated one-third holdout estimator. All the estimators were set to require about the same amount of computation. In the simulation study, the repeated 10-fold cross-validation estimator was found to have better performance than the .632+ bootstrap estimator when the classifier is highly adaptive to the training sample. We have also found that the .632+ bootstrap estimator suffers from a bias problem for large samples as well as for small samples.},
author = {Kim, Ji-Hyun},
doi = {10.1016/j.csda.2009.04.009},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/FG7REHPX/Kim - 2009 - Estimating classification error rate Repeated cro.pdf:pdf},
issn = {0167-9473},
journal = {Computational Statistics \& Data Analysis},
month = sep,
number = {11},
pages = {3735--3745},
shorttitle = {Estimating classification error rate},
title = {{Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947309001601 http://www.sciencedirect.com/science/article/pii/S0167947309001601/pdfft?md5=c4f42f7bd1f61cd0c3c977c58db9e1f5\&pid=1-s2.0-S0167947309001601-main.pdf},
volume = {53},
year = {2009}
}
@article{Chatterjee2007,
abstract = {The Anscombe dataset is popular for teaching the importance of graphics in data analysis. It consists of four datasets that have identical summary statistics (e.g., mean, standard deviation, and correlation) but dissimilar data graphics (scatterplots). In this article, we provide a general procedure to generate datasets with identical summary statistics but dissimilar graphics by using a genetic algorithm based approach.},
author = {Chatterjee, Sangit and Firat, Aykut},
doi = {10.1198/000313007X220057},
file = {:Users/arthur/Library/Application Support/Mendeley Desktop/Downloaded/A24ZHTXS/Chatterjee and Firat - 2007 - Generating Data with Identical Statistics but Diss.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
number = {3},
pages = {248--254},
title = {{Generating Data with Identical Statistics but Dissimilar Graphics}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313007X220057 http://www.tandfonline.com/doi/pdf/10.1198/000313007X220057},
volume = {61},
year = {2007}
}
@article{Arlot2010a,
abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Model selection,cross-validation,leave-one-out},
language = {EN},
mendeley-tags = {Model selection,cross-validation,leave-one-out},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Yen1956,
abstract = {The purpose of this investigation is to examine four special nonuniform sampling processes in detail, and to deduce some interesting properties of bandwidth-limited signals. The main results are contained in four generalized sampling theorems. These theorems not only contain the nature of determination (unique-specification, over-specification, and underspecification) of signals but also include explicit reconstruction formulas. From the reconstruction formulas, the complexity and accuracy of the nonuniform sampling processes discussed can be estimated. In addition, these theorems lead to observations regarding the allowable shapes, the "prediction," and the "energy" of bandwidth-limited signals in general. A "minimum-energy" signal is introduced which has certain advantages as compared to the ordinary "time-limited" signals when a finite number of sample values are given. Finally, a statement due to Cauchy on the sampling of bandwidth-limited signals is generalized to include a wider class of nonuniform sample point distributions and modified to give more exact information regarding the nature of determination of signals.},
author = {Yen, J.},
doi = {10.1109/TCT.1956.1086325},
issn = {0096-2007},
journal = {IRE Transactions on Circuit Theory},
keywords = {Bridges,Frequency conversion,Mathematics,Nonuniform sampling,Physics,Sampling methods,Signal sampling,Telegraphy,interpolation,signal processing},
mendeley-tags = {Bridges,Frequency conversion,Mathematics,Nonuniform sampling,Physics,Sampling methods,Signal sampling,Telegraphy,interpolation,signal processing},
number = {4},
pages = {251--257},
title = {{On Nonuniform Sampling of Bandwidth-Limited Signals}},
url = {http://ieeexplore.ieee.org/ielx5/8148/23600/01086325.pdf?tp=\&arnumber=1086325\&isnumber=23600 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1086325\&sortType=asc\_p\_Sequence\&filter=AND(p\_IS\_Number:23600)},
volume = {3},
year = {1956}
}

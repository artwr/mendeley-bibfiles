Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Bozdogan2000a,
abstract = {In this paper we briefly study the basic idea of Akaike's (1973) information criterion (AIC). Then, we present some recent developments on a new entropic or information complexity (ICOMP) criterion of Bozdogan (1988a, 1988b, 1990, 1994d, 1996, 1998a, 1998b) for model selection. A rationale for ICOMP as a model selection criterion is that it combines a badness-of-fit term (such as minus twice the maximum log likelihood) with a measure of complexity of a model differently than AIC, or its variants, by taking into account the interdependencies of the parameter estimates as well as the dependencies of the model residuals. We operationalize the general form of ICOMP based on the quantification of the concept of overall model complexity in terms of the estimated inverse-Fisher information matrix. This approach results in an approximation to the sum of two Kullback-Leibler distances. Using the correlational form of the complexity, we further provide yet another form of ICOMP to take into account the interdependencies (i.e., correlations) among the parameter estimates of the model. Later, we illustrate the practical utility and the importance of this new model selection criterion by providing several real as well as Monte Carlo simulation examples and compare its performance against AIC, or its variants. Copyright 2000 Academic Press.},
author = {Bozdogan, H},
doi = {10.1006/jmps.1999.1277},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {62--91},
pmid = {10733858},
title = {{Akaike's Information Criterion and Recent Developments in Information Complexity.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733858},
volume = {44},
year = {2000}
}
@article{Bozdogan1987,
author = {Bozdogan, Hamparsum},
doi = {10.1007/BF02294361},
issn = {0033-3123},
journal = {Psychometrika},
keywords = {AIC,Akaike,Behavioral Science,Information Criterion,Model Selection},
month = sep,
number = {3},
pages = {345--370},
publisher = {Springer New York},
title = {{Model selection and Akaike's Information Criterion (AIC): The general theory and its analytical extensions}},
url = {http://www.springerlink.com/content/442v373217p6363h/},
volume = {52},
year = {1987}
}
@article{Forster2000,
abstract = {What is model selection? What are the goals of model selection? What are the methods of model selection and how do they work? Which methods perform better than others and in what circumstances? These questions rest on a number of key concepts in a relatively underdeveloped field. The aim of this paper is to explain some background concepts, to highlight some of the results in this special issue, and to add my own. The standard methods of model selection include classical hypothesis testing, maximum likelihood, Bayes method, minimum description length, cross-validation, and Akaike's information criterion. They all provide an implementation of Occam's razor, in which parsimony or simplicity is balanced against goodness-of-fit. These methods primarily take account of the sampling errors in parameter estimation, although their relative success at this task depends on the circumstances. However, the aim of model selection should also include the ability of a model to generalize to predictions in a different domain. Errors of extrapolation, or generalization, are different from errors of parameter estimation. So, it seems that simplicity and parsimony may be an additional factor in managing these errors, in which case the standard methods of model selection are incomplete implementations of Occam's razor. Copyright 2000 Academic Press.},
author = {Forster, Mr},
doi = {10.1006/jmps.1999.1284},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {205--231},
pmid = {10733865},
title = {{Key Concepts in Model Selection: Performance and Generalizability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733865},
volume = {44},
year = {2000}
}
@article{Busemeyer2000,
abstract = {The purpose of this article is to formalize the generalization criterion method for model comparison. The method has the potential to provide powerful comparisons of complex and nonnested models that may also differ in terms of numbers of parameters. The generalization criterion differs from the better known cross-validation criterion in the following critical procedure. Although both employ a calibration stage to estimate parameters, cross-validation employs a replication sample from the same design for the validation stage, whereas generalization employs a new design for the critical stage. Two examples of the generalization criterion method are presented that demonstrate its usefulness for selecting a model based on sound scientific principles out of a set that also contains models lacking sound scientific principles that are either overly complex or oversimplified. The main advantage of the generalization criterion is its reliance on extrapolations to new conditions. After all, accurate a priori predictions to new conditions are the hallmark of a good scientific theory. Copyright 2000 Academic Press.},
author = {Busemeyer, Jr and Wang, Ym},
doi = {10.1006/jmps.1999.1282},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {171--189},
pmid = {10733863},
title = {{Model Comparisons and Model Selections Based on Generalization Criterion Methodology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733863},
volume = {44},
year = {2000}
}
@article{Golden2000,
abstract = {Model selection criteria (MSC) involves selecting the model with the best estimated goodness-of-fit to the data generating process. Following the method of Vuong (1989), a large sample Model Selection Test (MST), is introduced that can be used in conjunction with most existing MSC procedures to decide if the estimated goodness-of-fit for one model is significantly different from the estimated goodness-of-fit for another model. The MST extends the classical generalized likelihood ratio test, is valid in the presence of model misspecification, and is applicable to situations involving nonnested probability models. Simulation studies designed to illustrate the concept of the MST and its conservative decision rule (relative to the MSC method) are also presented. Copyright 2000 Academic Press.},
author = {Golden, Rm},
doi = {10.1006/jmps.1999.1281},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {153--170},
pmid = {10733862},
title = {{Statistical Tests for Comparing Possibly Misspecified and Nonnested Models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733862},
volume = {44},
year = {2000}
}
@article{Zucchini2000,
abstract = {This paper is an introduction to model selection intended for nonspecialists who have knowledge of the statistical concepts covered in a typical first (occasionally second) statistics course. The intention is to explain the ideas that generate frequentist methodology for model selection, for example the Akaike information criterion, bootstrap criteria, and cross-validation criteria. Bayesian methods, including the Bayesian information criterion, are also mentioned in the context of the framework outlined in the paper. The ideas are illustrated using an example in which observations are available for the entire population of interest. This enables us to examine and to measure effects that are usually invisible, because in practical applications only a sample from the population is observed. The problem of selection bias, a hazard of which one needs to be aware in the context of model selection, is also discussed. Copyright 2000 Academic Press.},
author = {Zucchini, W},
doi = {10.1006/jmps.1999.1276},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {41--61},
pmid = {10733857},
title = {{An Introduction to Model Selection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733857},
volume = {44},
year = {2000}
}
@article{Pitt2002,
abstract = {How should we select among computational models of cognition? Although it is commonplace to measure how well each model fits the data, this is insufficient. Good fits can be misleading because they can result from properties of the model that have nothing to do with it being a close approximation to the cognitive process of interest (e.g. overfitting). Selection methods are introduced that factor in these properties when measuring fit. Their success in outperforming standard goodness-of-fit measures stems from a focus on measuring the generalizability of a model's data-fitting abilities, which should be the goal of model selection.},
author = {Pitt, Mark a. and Myung, In Jae},
issn = {1879-307X},
journal = {Trends in cognitive sciences},
month = oct,
number = {10},
pages = {421--425},
pmid = {12413575},
title = {{When a good fit can be bad.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12413575},
volume = {6},
year = {2002}
}
@article{Wasserman2000,
abstract = {This paper reviews the Bayesian approach to model selection and model averaging. In this review, I emphasize objective Bayesian methods based on noninformative priors. I will also discuss implementation details, approximations, and relationships to other methods. Copyright 2000 Academic Press.},
author = {Wasserman, L},
doi = {10.1006/jmps.1999.1278},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
keywords = {aic,bayes factors,bic,consistency,default bayes methods,markov chain monte carlo},
month = mar,
number = {1},
pages = {92--107},
pmid = {10733859},
title = {{Bayesian Model Selection and Model Averaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733859},
volume = {44},
year = {2000}
}
@article{Browne2000,
abstract = {This paper gives a review of cross-validation methods. The original applications in multiple linear regression are considered first. It is shown how predictive accuracy depends on sample size and the number of predictor variables. Both two-sample and single-sample cross-validation indices are investigated. The application of cross-validation methods to the analysis of moment structures is then justified. An equivalence of a single-sample cross-validation index and the Akaike information criterion is pointed out. It is seen that the optimal number of parameters suggested by both single-sample and two-sample cross-validation indices will depend on sample size. Copyright 2000 Academic Press.},
author = {Browne, Mw},
doi = {10.1006/jmps.1999.1279},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {108--132},
pmid = {10733860},
title = {{Cross-Validation Methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733860},
volume = {44},
year = {2000}
}
@article{Sewell2008,
abstract = {MOTIVATION: The analysis of multiple sequence alignments is allowing researchers to glean valuable insights into evolution, as well as identify genomic regions that may be functional, or discover novel classes of functional elements. Understanding the distribution of conservation levels that constitutes the evolutionary landscape is crucial to distinguishing functional regions from non-functional. Recent evidence suggests that a binary classification of evolutionary rates is inappropriate for this purpose and finds only highly conserved functional elements. Given that the distribution of evolutionary rates is multimodal, determining the number of modes is of paramount concern. Through simulation, we evaluate the performance of a number of information criterion approaches derived from MCMC simulations in determining the dimension of a model. RESULTS: We utilize a Deviance Information Criterion (DIC) approximation that is more robust than the approximations from other information criteria, and show our information criteria approximations do not produce superfluous modes when estimating conservation distributions under a variety of circumstances. We analyse the distribution of conservation for a multiple alignment comprising four primate species and mouse, and repeat this on two additional multiple alignments of similar species. We find evidence of six distinct classes of evolutionary rates that appear to be robust to the species used. AVAILABILITY: Source code and data are available at http://dl.dropbox.com/u/477240/changept.zip CONTACT: Jonathan.Keithmonash.edu.},
author = {Sewell, Martin},
doi = {10.1093/bioinformatics/btq716},
editor = {Lahiri, P},
institution = {University College London},
isbn = {0940600528},
issn = {13674811},
journal = {Symposium A Quarterly Journal In Modern Foreign Literatures},
number = {April 2007},
pages = {1--7},
pmid = {21208984},
publisher = {Institute of Mathematical Statistics},
title = {{Model Selection}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21293060},
volume = {2007},
year = {2008}
}
@article{Myung2000,
abstract = {Model selection should be based not solely on goodness-of-fit, but must also consider model complexity. While the goal of mathematical modeling in cognitive psychology is to select one model from a set of competing models that best captures the underlying mental process, choosing the model that best fits a particular set of data will not achieve this goal. This is because a highly complex model can provide a good fit without necessarily bearing any interpretable relationship with the underlying process. It is shown that model selection based solely on the fit to observed data will result in the choice of an unnecessarily complex model that overfits the data, and thus generalizes poorly. The effect of over-fitting must be properly offset by model selection methods. An application example of selection methods using artificial data is also presented. Copyright 2000 Academic Press.},
author = {Myung, Ij},
doi = {10.1006/jmps.1999.1283},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {190--204},
pmid = {10733864},
title = {{The Importance of Complexity in Model Selection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733864},
volume = {44},
year = {2000}
}
@article{Cutting2000,
abstract = {Traditionally, models are compared on the basis of their accuracy, their scope, and their simplicity. Simplicity is often represented by parameter counts; the fewer the parameters, the simpler the model. Arguments are presented here suggesting that simplicity has little place in discussions of modeling; instead, the concept of flexibility should be substituted. When comparing two models one should be wary of the possibility of their differential flexibility. Several methods for assessing relative flexibility are possible, as represented in this special issue of the Journal of Mathematical Psychology. Here, the method of cross-validation is applied in the comparison of two models, a linear integration model (LIM) and the fuzzy-logical model of perception (FLMP), in the fitting of 44 data sets concerning the perception of layout seen among three panels with the presence or absence of four sources of information for depth. Prior to cross-validation the two models performed about equally well; after cross-validation LIM was statistically superior to FLMP, but the overall pattern of fits remained nearly the same for both models. Copyright 2000 Academic Press.},
author = {Cutting, Je},
doi = {10.1006/jmps.1999.1274},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {3--19},
pmid = {10733855},
title = {{Accuracy, Scope, and Flexibility of Models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733855},
volume = {44},
year = {2000}
}
@article{Grunwald2000,
abstract = {We introduce the minimum description length (MDL) principle, a general principle for inductive inference based on the idea that regularities (laws) underlying data can always be used to compress data. We introduce the fundamental concept of MDL, called the stochastic complexity, and we show how it can be used for model selection. We briefly compare MDL-based model selection to other approaches and we informally explain why we may expect MDL to give good results in practical applications. Copyright 2000 Academic Press.},
author = {Gr\"{u}nwald, P},
doi = {10.1006/jmps.1999.1280},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {133--152},
pmid = {10733861},
title = {{Model Selection Based on Minimum Description Length.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733861},
volume = {44},
year = {2000}
}
@article{Walker2006,
author = {Walker, D and Marion, G},
doi = {10.1016/j.physd.2005.11.007},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
keywords = {description length,model selection,nonlinear models,stochastic process models},
month = jan,
number = {2},
pages = {190--196},
title = {{Selecting nonlinear stochastic process rate models using information criteria}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167278905004847},
volume = {213},
year = {2006}
}
@article{Bamber2000,
abstract = {Formal definitions are given of the following intuitive concepts: (a) A model is quantitatively testable if its predictions are highly precise and narrow. (b) A model is identifiable if the values of its parameters can be ascertained from empirical observations. (c) A model is redundant if the values of some parameters can be deduced from others or if the values of some observables can be deduced from others. Various rules of thumb for nonredundant models are examined. The Counting Rule states that a model is quantitatively testable if and only if it has fewer parameters than observables. This rule can be safely applied only to identifiable models. If a model is unidentifiable, one must apply a generalization of the Counting Rule known as the Jacobian Rule. This rule states that a model is quantitatively testable if and only if the maximum rank (i.e., the number of linearly independent columns) of its Jacobian matrix (i.e., the matrix of partial derivatives of the function that maps parameter values to the predicted values of observables) is smaller than the number of observables. The Identifiability Rule states that a model is identifiable if and only if the maximum rank of its Jacobian matrix equals the number of parameters. The conclusions provided by these rules are only presumptive. To reach definitive conclusions, additional analyses must be performed. To illustrate the foregoing, the quantitative testability and identifiability of linear models and of discrete-state models are analyzed. Copyright 2000 Academic Press.},
author = {Bamber, D and {van Santen JP}},
doi = {10.1006/jmps.1999.1275},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
month = mar,
number = {1},
pages = {20--40},
pmid = {10733856},
title = {{How to Assess a Model's Testability and Identifiability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733856},
volume = {44},
year = {2000}
}
